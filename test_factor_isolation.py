#!/usr/bin/env python3
"""
Factor Isolation Test - ë‹¤ìš´ìƒ˜í”Œë§ vs Inference Steps ì˜í–¥ ë¶„ë¦¬ í…ŒìŠ¤íŠ¸

í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤:
1. Baseline: ë‹¤ìš´ìƒ˜í”Œë§ OFF + steps=12 (ì›ë³¸)
2. Test A: ë‹¤ìš´ìƒ˜í”Œë§ ON (768px) + steps=12 (ë‹¤ìš´ìƒ˜í”Œë§ë§Œ ì ìš©)
3. Test B: ë‹¤ìš´ìƒ˜í”Œë§ OFF + steps=8 (stepsë§Œ ê°ì†Œ)

ê° ìš”ì†Œê°€ ë¶€í”¼ ì •í™•ë„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ë¦¬í•˜ì—¬ ì¸¡ì •
"""

import asyncio
import time
import os
import sys
import json
import numpy as np
from PIL import Image

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))


async def run_test_with_config(config_name: str, max_size: str, steps: str, test_image: str, gpu_ids: list):
    """íŠ¹ì • ì„¤ì •ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print(f"\n{'='*60}")
    print(f"[{config_name}] MAX_IMAGE_SIZE={max_size}, STEPS={steps}")
    print(f"{'='*60}")

    # ì›Œì»¤ íŒŒì¼ ìˆ˜ì •
    worker_file = "ai/subprocess/persistent_3d_worker.py"

    with open(worker_file, "r") as f:
        original_content = f.read()

    # ì„¤ì • ë³€ê²½
    modified_content = original_content

    # MAX_IMAGE_SIZE ë³€ê²½
    if "MAX_IMAGE_SIZE = 768" in modified_content:
        modified_content = modified_content.replace(
            "MAX_IMAGE_SIZE = 768",
            f"MAX_IMAGE_SIZE = {max_size}"
        )
    elif "MAX_IMAGE_SIZE = None" in modified_content:
        modified_content = modified_content.replace(
            "MAX_IMAGE_SIZE = None",
            f"MAX_IMAGE_SIZE = {max_size}"
        )

    # STAGE2_INFERENCE_STEPS ë³€ê²½
    if "STAGE2_INFERENCE_STEPS = 10" in modified_content:
        modified_content = modified_content.replace(
            "STAGE2_INFERENCE_STEPS = 10",
            f"STAGE2_INFERENCE_STEPS = {steps}"
        )
    elif "STAGE2_INFERENCE_STEPS = 12" in modified_content:
        modified_content = modified_content.replace(
            "STAGE2_INFERENCE_STEPS = 12",
            f"STAGE2_INFERENCE_STEPS = {steps}"
        )
    elif "STAGE2_INFERENCE_STEPS = 8" in modified_content:
        modified_content = modified_content.replace(
            "STAGE2_INFERENCE_STEPS = 8",
            f"STAGE2_INFERENCE_STEPS = {steps}"
        )

    with open(worker_file, "w") as f:
        f.write(modified_content)

    print(f"Config applied: MAX_IMAGE_SIZE={max_size}, STEPS={steps}")

    try:
        from ai.gpu import SAM3DWorkerPool, initialize_gpu_pool, get_gpu_pool
        from ai.pipeline import FurniturePipeline
        import ai.gpu.sam3d_worker_pool as sam3d_module

        # ê¸°ì¡´ í’€ì´ ìˆìœ¼ë©´ ì´ˆê¸°í™”
        try:
            existing_pool = get_gpu_pool()
        except:
            pass

        gpu_pool = initialize_gpu_pool(gpu_ids)

        # Worker Pool ì‹œì‘
        sam3d_pool = SAM3DWorkerPool(gpu_ids=gpu_ids, init_timeout=180.0)
        await sam3d_pool.start_workers()

        sam3d_module._global_sam3d_pool = sam3d_pool

        pipeline = FurniturePipeline(
            sam2_api_url="http://localhost:8000",
            enable_3d_generation=True,
            device_id=0,
            gpu_pool=gpu_pool
        )

        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        image_url = f"file://{os.path.abspath(test_image)}"
        start = time.time()

        result = await pipeline.process_single_image(
            image_url=image_url,
            enable_mask=True,
            enable_3d=True,
            use_parallel_3d=True
        )

        elapsed = time.time() - start

        # ê²°ê³¼ ìˆ˜ì§‘
        volumes = []
        for obj in result.objects:
            if obj.relative_dimensions:
                volumes.append({
                    "label": obj.label,
                    "volume": obj.relative_dimensions.get("volume", 0)
                })

        print(f"Time: {elapsed:.2f}s, Objects: {len(volumes)}")

        # ì›Œì»¤ ì¢…ë£Œ
        await sam3d_pool.shutdown()

        return {
            "config": config_name,
            "max_size": max_size,
            "steps": steps,
            "time": elapsed,
            "volumes": volumes
        }

    finally:
        # ì›ë³¸ ë³µì›
        with open(worker_file, "w") as f:
            f.write(original_content)


async def main():
    print("=" * 70)
    print("Factor Isolation Test - ë‹¤ìš´ìƒ˜í”Œë§ vs Inference Steps ì˜í–¥ ë¶„ë¦¬")
    print("=" * 70)

    import torch
    gpu_ids = list(range(min(4, torch.cuda.device_count()))) if torch.cuda.is_available() else [0]
    print(f"Using GPUs: {gpu_ids}")

    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì„ íƒ (1ê°œë§Œ ì‚¬ìš©)
    test_image = "ai/imgs/bed-1834327_1920.jpg"
    print(f"Test image: {test_image}")

    results = {}

    # Test 1: Baseline (ë‹¤ìš´ìƒ˜í”Œë§ OFF + steps=12)
    results["baseline"] = await run_test_with_config(
        "Baseline", "None", "12", test_image, gpu_ids
    )

    # Test 2: ë‹¤ìš´ìƒ˜í”Œë§ë§Œ ì ìš© (768px + steps=12)
    results["downsample_only"] = await run_test_with_config(
        "Downsample Only", "768", "12", test_image, gpu_ids
    )

    # Test 3: Stepsë§Œ ê°ì†Œ (ë‹¤ìš´ìƒ˜í”Œë§ OFF + steps=8)
    results["steps_only"] = await run_test_with_config(
        "Steps Only", "None", "8", test_image, gpu_ids
    )

    # ê²°ê³¼ ë¶„ì„
    print("\n" + "=" * 70)
    print("[Results Analysis] ìš”ì†Œë³„ ì˜í–¥ ë¶„ì„")
    print("=" * 70)

    baseline_vols = {v["label"]: v["volume"] for v in results["baseline"]["volumes"]}
    downsample_vols = {v["label"]: v["volume"] for v in results["downsample_only"]["volumes"]}
    steps_vols = {v["label"]: v["volume"] for v in results["steps_only"]["volumes"]}

    print(f"\n{'ê°ì²´':<20} {'Baseline':<12} {'ë‹¤ìš´ìƒ˜í”Œë§':<12} {'Stepsê°ì†Œ':<12} {'ë‹¤ìš´ìƒ˜í”Œë§ì°¨ì´':<15} {'Stepsì°¨ì´':<15}")
    print("-" * 90)

    downsample_diffs = []
    steps_diffs = []

    for label in baseline_vols:
        baseline_v = baseline_vols.get(label, 0)
        downsample_v = downsample_vols.get(label, 0)
        steps_v = steps_vols.get(label, 0)

        if baseline_v > 0:
            downsample_diff = abs(baseline_v - downsample_v) / baseline_v * 100
            steps_diff = abs(baseline_v - steps_v) / baseline_v * 100

            downsample_diffs.append(downsample_diff)
            steps_diffs.append(steps_diff)

            print(f"{label:<20} {baseline_v:<12.4f} {downsample_v:<12.4f} {steps_v:<12.4f} {downsample_diff:<15.1f}% {steps_diff:<15.1f}%")

    print("-" * 90)

    avg_downsample = np.mean(downsample_diffs) if downsample_diffs else 0
    avg_steps = np.mean(steps_diffs) if steps_diffs else 0
    max_downsample = np.max(downsample_diffs) if downsample_diffs else 0
    max_steps = np.max(steps_diffs) if steps_diffs else 0

    print(f"\n{'í‰ê·  ì°¨ì´:':<20} {'':<12} {'':<12} {'':<12} {avg_downsample:<15.1f}% {avg_steps:<15.1f}%")
    print(f"{'ìµœëŒ€ ì°¨ì´:':<20} {'':<12} {'':<12} {'':<12} {max_downsample:<15.1f}% {max_steps:<15.1f}%")

    print("\n" + "=" * 70)
    print("[Conclusion] ê²°ë¡ ")
    print("=" * 70)

    if avg_downsample > avg_steps:
        print(f"\nğŸ”´ ë‹¤ìš´ìƒ˜í”Œë§ì´ ë¶€í”¼ ì •í™•ë„ì— ë” í° ì˜í–¥ ({avg_downsample:.1f}% vs {avg_steps:.1f}%)")
        print("   â†’ ë‹¤ìš´ìƒ˜í”Œë§ì„ ë¹„í™œì„±í™”í•˜ê³  inference stepsë§Œ ì¡°ì •í•˜ëŠ” ê²ƒì„ ê¶Œì¥")
    elif avg_steps > avg_downsample:
        print(f"\nğŸ”´ Inference Steps ê°ì†Œê°€ ë¶€í”¼ ì •í™•ë„ì— ë” í° ì˜í–¥ ({avg_steps:.1f}% vs {avg_downsample:.1f}%)")
        print("   â†’ Stepsë¥¼ 12ë¡œ ìœ ì§€í•˜ê³  ë‹¤ìš´ìƒ˜í”Œë§ë§Œ ì ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥")
    else:
        print(f"\nğŸŸ¡ ë‘ ìš”ì†Œì˜ ì˜í–¥ì´ ë¹„ìŠ·í•¨ ({avg_downsample:.1f}% vs {avg_steps:.1f}%)")

    # ì²˜ë¦¬ ì‹œê°„ ë¹„êµ
    print(f"\nì²˜ë¦¬ ì‹œê°„ ë¹„êµ:")
    print(f"  Baseline: {results['baseline']['time']:.2f}s")
    print(f"  ë‹¤ìš´ìƒ˜í”Œë§ë§Œ: {results['downsample_only']['time']:.2f}s (ì°¨ì´: {results['baseline']['time'] - results['downsample_only']['time']:.2f}s)")
    print(f"  Stepsê°ì†Œë§Œ: {results['steps_only']['time']:.2f}s (ì°¨ì´: {results['baseline']['time'] - results['steps_only']['time']:.2f}s)")

    # ê²°ê³¼ ì €ì¥
    with open("test_factor_isolation_results.json", "w") as f:
        json.dump({
            "results": results,
            "analysis": {
                "avg_downsample_diff": avg_downsample,
                "avg_steps_diff": avg_steps,
                "max_downsample_diff": max_downsample,
                "max_steps_diff": max_steps,
                "conclusion": "downsample" if avg_downsample > avg_steps else "steps"
            }
        }, f, indent=2)

    print("\nResults saved to: test_factor_isolation_results.json")


if __name__ == "__main__":
    asyncio.run(main())
